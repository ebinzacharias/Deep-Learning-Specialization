{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Project\n",
    "\n",
    "### Machine Learning Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ml-development-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas to improve the the system after training\n",
    "\n",
    "1. Collect more data\n",
    "2. more diverse data\n",
    "3. train longer with gradient descent\n",
    "4. try different optimization algorithm\n",
    "5. Try a bigger network or small\n",
    "6. Try dropout or L2 regularization\n",
    "\n",
    "Choosing the correct idea is very critical !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of assumptions in ML\n",
    "\n",
    "1. Fit Training set well on cost function\n",
    "\n",
    "    > Train a bigger network or switch to a better optimization algorithm\n",
    "    \n",
    "2. Fit dev set well on cost function\n",
    "    \n",
    "    > Regularization\n",
    "    > Bigger Training set\n",
    "    \n",
    "3. Fit test set well on cost function\n",
    "\n",
    "    > Bigger Dev set \n",
    "    \n",
    "4. Performs well in real world\n",
    "\n",
    "    > Change either the dev set or cost function.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single number evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](single.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisficing and Optimizing metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hqdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you have N matrix that you care about it's sometimes reasonable to pick one of them to be optimizing. So you want to do as well as is possible on that one. And then N minus 1 to be satisficing, meaning that so long as they reach some threshold such as running times faster than 100 milliseconds, but so long as they reach some threshold, you don't care how much better it is in that threshold, but they have to reach that threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/dev/test set distributions\n",
    "\n",
    "choose a dev set and test set to reflect data you expect to get in future and consider important to do well on. \n",
    "\n",
    "And, in particular, the dev set and the test set here, should come from the same distribution.\n",
    "\n",
    "Setting up the dev set, as well as the validation metric, is really defining what target you want to aim at. And hopefully, by setting the dev set and the test set to the same distribution, you're really aiming at whatever target you hope your machine learning team will hit. \n",
    "\n",
    "### Size of the dev and test sets\n",
    "\n",
    "#### train/dev/test set\n",
    "\n",
    "Conventionally, 70/30 % or 70/20/20%\n",
    "\n",
    "However depending on the size of the dataset, this can be varied.\n",
    "\n",
    "Eg. Dataset = 1000000\n",
    "\n",
    "therefore, a 98/1/1% will be enough !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model Performance\n",
    "\n",
    "1. Fit the training set well \n",
    "    > Achieve low avoidable bias\n",
    "    \n",
    "2. Training set performance generalises pretty well in the dev/test set.\n",
    "    > Variance is not too bad\n",
    "    \n",
    "\n",
    "#### Reducing (avoidable) bias and variance\n",
    "\n",
    "Human level\n",
    "\n",
    "> Avoidable Bias\n",
    "\n",
    "    Train bigger model\n",
    "    Train longer/better optimization algorithm - momentum, rmsprop, adam etc\n",
    "    Better NN architecture / hyperparameters search - RNN, CNN\n",
    "    \n",
    "\n",
    "Training Error\n",
    "\n",
    "> Variance\n",
    "\n",
    "    More Data - generalises better\n",
    "    regularization - L2, dropout, data augmentation\n",
    "    Better NN architecture / hyperparameters search - RNN, CNN\n",
    "\n",
    "Dev Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ceiling Analysis\n",
    "\n",
    "Check mislabeled results (eg. classification problem)\n",
    "\n",
    "Consider all the incorrect cases and possible improvements and account for the percentage of error of each.\n",
    "Include incorrect labels as a column to analyse.\n",
    "\n",
    "Work on improving problems that has significant improvement potential, thus increasing the overall accuracy o the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your first system quickly, then iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing on different distributions\n",
    "\n",
    "### Option 1\n",
    "\n",
    "1. Take images from both the distributions and randomly shuffle them into a train, dev, and test set. \n",
    "\n",
    "#### advantage \n",
    "\n",
    "dev and test sets will all come from the same distribution, so that makes it easier to manage.\n",
    "\n",
    "#### huge disadvantage, \n",
    "\n",
    "dev set, large amount of data can be from one distribution which is not your target distribution.\n",
    "\n",
    "### Option 2\n",
    "\n",
    "Training set = both distribution  \n",
    "dev set and test set = target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance with mismatched data distributions\n",
    "\n",
    "Human error  \n",
    "|  \n",
    "##### Avoidable Bias\n",
    "|    \n",
    "Train error  \n",
    "|  \n",
    "##### Variance problem\n",
    "|  \n",
    "Train-val error  \n",
    "|  \n",
    "##### Data mismatch problem\n",
    "|  \n",
    "Val error  \n",
    "|\n",
    "##### Degree of overfitting to Val set  \n",
    "|  \n",
    "Test error  \n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Train set\n",
    "Train-val set\n",
    "Val set\n",
    "Test set\n",
    "\n",
    "#### Variance Problem \n",
    "##### Example\n",
    "\n",
    "Train error      = 1%  \n",
    "Train-val error  = 9%  \n",
    "Val error        = 10%  \n",
    "\n",
    "#### Data Mismatch Problem \n",
    "##### Example\n",
    "\n",
    "Train error      = 1%  \n",
    "Train-val error  = 1.5%  \n",
    "Val error        = 10%  \n",
    "\n",
    "#### Bias Problem (Avoidable) \n",
    "##### Example\n",
    "\n",
    "Human error      = 0%\n",
    "Train error      = 10%  \n",
    "Train-val error  = 11%  \n",
    "Val error        = 12%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing data mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. carry out manual error analysis and try to understand the differences between the training set and the dev/test sets. To avoid overfitting the test set, technically for error analysis, you should manually only look at a dev set and not at the test set. \n",
    "\n",
    "##### When you have insight into the nature of the dev set errors, or you have insight into how the dev set may be different or harder than your training set\n",
    "\n",
    "2. make the training data more similar.\n",
    "\n",
    "###### Artificial Data Synthesis\n",
    "be cautious and bear in mind whether or not you might be accidentally simulating data only from a tiny subset of the space of all possible examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "take this last output layer of the neural network and just delete that and delete also the weights feeding into that last output layer and create a new set of randomly initialized weights just for the last layer and have that as output now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](TL.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining vs fine tuning\n",
    "\n",
    "You have a model m.\n",
    "#### Pre-training: \n",
    "You have a dataset A on which you train m.\n",
    "You have a dataset B. Before you start training the model, you initialize some of the parameters of m with the model which is trained on A.\n",
    "#### Fine-tuning:  \n",
    "You train m on B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end DL\n",
    "\n",
    "![](end.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](pros.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
