{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Computer stores three separate matrices corresponding to the red, green, and blue color channels of the image.\n",
    "\n",
    "##### pixel intensity values - Into a feature vector\n",
    "\n",
    "The pixel values are unrolled to a feature vector x.\n",
    "So the total dimension of this vector x will be (x pixels* y pixels * 3) (nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "(x,y) represents a single training example\n",
    "\n",
    "where x = feature vector\n",
    "      y = label (0 or 1 in this case)\n",
    "    \n",
    "m = number of training examples\n",
    "  = {(x^(1), y^(1)), (x^(2), y^(2)) ....... (x^(m), y^(m))}\n",
    "    \n",
    "mtrain,mtest \n",
    "\n",
    "X = column matrix with all the training examples\n",
    "X = m * nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "#### Logistic regression is an algorithm for binary classification.\n",
    "\n",
    "Logistic Regression is used when the dependent variable(target) is categorical.\n",
    "\n",
    "For example,\n",
    "To predict whether an email is spam (1) or (0)\n",
    "Whether the tumor is malignant (1) or not (0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression, this isn't a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. So Y hat should really be between zero and one, and it's difficult to enforce that because W transpose X plus B can be much bigger than one or it can even be negative, which doesn't make sense for probability. That you want it to be between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence, logistic regression uses the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given,\n",
    "\n",
    "input feature vector X\n",
    "\n",
    "we want the output, \n",
    "\n",
    "### $\\hat{y}$ such as $\\hat{y}$ = P(y=1|x)\n",
    "\n",
    "### parameters : w and b  \n",
    "### W, an n<sub>x</sub> dimensional vector and b a real number.\n",
    "\n",
    "### $\\hat{y}$ = Ïƒ(w<sup>T</sup> + b)  \n",
    "\n",
    "#### if t is very large, \n",
    "\n",
    "then E to the negative t will be close to zero. So then sigmoid of t will be approximately one over one plus something very close to zero, because E to the negative of very large number will be close to zero. So this is close to 1. \n",
    "\n",
    "#### Conversely, if t is very small, or it is a very large negative number,   \n",
    "then sigmoid of t becomes one over one plus E to the negative t, and this becomes, it's a huge number. So this becomes, think of it as one over one plus a number that is very, very big, and so, that's close to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss function computes the error for a single training example \n",
    "\n",
    "#### The cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "the cost function measures how well your parameters w and b are doing on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "##### A convex function does not have multiple local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function J(w,b,) is, then, some surface above these horizontal axes w and b. So the height of the surface represents the value of J(w,b) at a certain point. And what we want to do is really to find the value of w and b that corresponds to the minimum of the cost function J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find a good value for the parameters, what we'll do is initialize w and b to some initial value, maybe denoted by that little red dot. And for logistic regression almost any initialization method works, usually you initialize the value to zero. Random initialization also works, but people don't usually do that for logistic regression. But because this function is convex, no matter where you initialize, you should get to the same point or roughly the same point. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gradient.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
